{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "0.6.0\n",
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "\n",
    "import numpy as np\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=14.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "from torchvision import  transforms\n",
    "import collections\n",
    "\n",
    "# path to your own data and coco file\n",
    "train_data_dir = '/data/cmpe256-01-su2020/coco/train2017'\n",
    "val_data_dir = '/data/cmpe256-01-su2020/coco/val2017'\n",
    "train_coco = '/data/cmpe256-01-su2020/coco/annotations/instances_train2017.json'\n",
    "val_coco = '/data/cmpe256-01-su2020/coco/annotations/instances_val2017.json'\n",
    "transform = transforms.Compose([transforms.Resize((480, 640)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                               ])\n",
    "\n",
    "coco_val = dset.CocoDetection(root = val_data_dir,\n",
    "                                annFile = val_coco,\n",
    "                                transform=transform)\n",
    "\n",
    "coco_train = dset.CocoDetection(root = train_data_dir,\n",
    "                                annFile = train_coco,\n",
    "                                transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(coco_train, batch_size=12)\n",
    "val_loader = torch.utils.data.DataLoader(coco_train, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  118287\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: ', len(coco_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# our dataset has two classes only - raccoon and not racoon\n",
    "num_classes = 2\n",
    "# get the model using our helper function\n",
    "model = models.mobilenet_v2(pretrained=False)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation\n",
      "[[tensor([500.4900, 437.5200, 267.3800,   4.7800, 164.5000, 382.4800, 181.5900,\n",
      "        261.2000,  52.3600, 468.7300, 166.8300, 134.3600], dtype=torch.float64), tensor([473.5300, 353.3300, 330.1400,  22.9200, 479.3800, 268.6300, 363.4300,\n",
      "        251.7100, 387.7700, 228.3600, 509.1200, 145.5500], dtype=torch.float64), tensor([599.7300, 437.8700, 281.8100,  75.4500, 120.2600, 330.2400, 186.8000,\n",
      "        270.8200, 100.6800, 468.7300, 240.1800, 117.0200], dtype=torch.float64), tensor([419.6000, 326.9800, 314.7500,  39.1600, 448.4000, 229.9300, 362.1300,\n",
      "        259.0400, 391.4800, 224.7300, 445.8400, 155.6700], dtype=torch.float64), tensor([612.6700, 433.6500, 299.1200, 148.0300,  93.7000, 278.9700, 186.8000,\n",
      "        274.9400, 123.8100, 473.2600, 256.0000, 115.5800], dtype=torch.float64), tensor([375.3700, 306.2600, 282.0500,  46.8000, 442.8700, 205.7500, 354.9700,\n",
      "        259.0400, 395.2000, 222.5500, 418.5200, 168.6700], dtype=torch.float64), tensor([608.3600, 427.3300, 281.8100, 185.2800,  91.5000, 228.6600, 182.8900,\n",
      "        267.6100, 124.2300, 479.7900, 294.8300, 119.9100], dtype=torch.float64), tensor([354.8800, 287.2900, 258.9600,  28.6500, 440.6500, 143.8300, 341.9400,\n",
      "        260.4100, 452.6100, 217.1100, 389.7500, 191.7800], dtype=torch.float64), tensor([528.5400, 427.3300, 248.1400, 212.0200,  36.1700, 214.1500, 178.9900,\n",
      "        267.1600, 130.0100, 484.1500, 299.1500, 151.6900], dtype=torch.float64), tensor([269.6600, 279.5600, 224.3400,  20.0600, 383.1200, 140.9300, 335.4200,\n",
      "        263.1600, 456.7400, 216.9300, 371.0600, 219.2300], dtype=torch.float64), tensor([457.3500, 424.5200, 242.3700, 247.3600,  28.4300, 225.7600, 189.4100,\n",
      "        272.6500, 128.7700, 494.1200, 310.6500, 163.2500], dtype=torch.float64), tensor([201.7100, 266.2100, 189.7100,  29.6100, 384.2300, 134.1600, 334.7700,\n",
      "        262.2400, 502.5800, 217.6600, 317.8400, 227.9000], dtype=torch.float64), tensor([420.6700, 417.8500, 213.5200, 271.2400,  35.0600, 257.6900, 193.9700,\n",
      "        277.2300, 127.9400, 495.5700, 245.9300, 158.9200], dtype=torch.float64), tensor([187.6900, 255.6800, 186.8300,  40.1100, 382.0200, 123.5200, 334.7700,\n",
      "        261.7900, 522.1100, 220.0100, 247.3700, 249.5700], dtype=torch.float64), tensor([389.3900, 417.4900, 237.5600, 285.5600,   4.1000, 277.0300, 199.1800,\n",
      "        280.4400,  52.3600, 485.6000, 162.5200, 124.2400], dtype=torch.float64), tensor([192.0000, 248.6500, 160.8600,  44.8900, 307.8900,  82.8900, 339.3300,\n",
      "        255.3700, 512.6200, 221.2800, 212.8500, 246.6800], dtype=torch.float64)]]\n",
      "area\n",
      "tensor([1.2006e+05, 1.9687e+04, 4.7676e+04, 9.2920e+04, 9.7487e+04, 5.3482e+04,\n",
      "        8.4512e+03, 9.1436e+02, 1.5185e+04, 1.6183e+02, 8.0169e+04, 1.7202e+05],\n",
      "       dtype=torch.float64)\n",
      "iscrowd\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "image_id\n",
      "tensor([ 9, 25, 30, 34, 36, 42, 49, 61, 64, 71, 72, 73])\n",
      "bbox\n",
      "[tensor([  1.0800, 385.5300, 204.8600,   0.9600,   0.0000, 214.1500, 162.5700,\n",
      "        261.2000,  52.3600, 467.8200, 136.6300,  13.0000], dtype=torch.float64), tensor([187.6900,  60.0300,  31.0200,  20.0600,  50.1200,  41.2900, 226.5600,\n",
      "        205.9200, 387.7700, 216.9300, 129.4400,  22.7500], dtype=torch.float64), tensor([611.5900, 214.9700, 254.8800, 441.2300, 457.6800, 348.2600, 130.4100,\n",
      "         48.0800, 176.3600,  27.7500, 289.0800, 535.9800], dtype=torch.float64), tensor([285.8400, 297.1600, 324.1200, 379.1500, 430.3500, 243.7800, 184.4300,\n",
      "         57.2400, 157.6200,  14.1500, 499.0500, 609.6700], dtype=torch.float64)]\n",
      "category_id\n",
      "tensor([51, 25, 64, 24, 28, 18, 19,  1,  3,  3, 25,  4])\n",
      "id\n",
      "tensor([1038967,  598548,  291613,  589229,  284996, 1817255,   56407,  434050,\n",
      "         350246,  141916,  598731,  246920])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-207d8dbe81e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         print (target[0]['bbox'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#         print (torch.stack(target[0]['bbox']).size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'area'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-3.6.6-gpu/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-3.6.6-gpu/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-3.6.6-gpu/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Double"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader: #one batch data\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "#         print (output.size())\n",
    "#         print (output)\n",
    "        for x in target[0]:\n",
    "            print (x)\n",
    "            print (target[0][x])\n",
    "#         print(target[0]['segmentation'][0])\n",
    "#         print (target[0]['bbox'])\n",
    "#         print (torch.stack(target[0]['bbox']).size())\n",
    "        loss = criterion(output, target[0]['area'])\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)#batch size\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)#calculate average loss\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
